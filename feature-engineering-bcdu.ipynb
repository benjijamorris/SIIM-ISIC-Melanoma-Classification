{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algera\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nfrom torch import FloatTensor\nimport time\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport cv2\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, BatchNormalization,  Dropout , Activation\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\n\ndef build_prediction_model():\n\n    model = Sequential()\n        \n    model.add(Dense(2048, input_dim = 785))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(512))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(128))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(32))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    \n    model.add(Dense(1, activation = 'sigmoid'))\n\n    model.compile(loss='binary_crossentropy',#[binary_focal_loss(alpha=.25, gamma=2)],\n                              optimizer='adam',\n                              metrics=[AUC(), 'accuracy', Precision(), Recall()])\n#     print(model.summary())\n    return model\n\n\nbasepath = \"../input/siim-isic-melanoma-classification/\"\n\nx,y = 256, 256\n\nbatch_size = 16\nvalid_size = 16\n\nseg_gen = ImageDataGenerator()\n\ndef normalize_df(filename = None, type = 'test', df = None ):\n    if  filename != None:\n        df = pd.read_csv(filename)\n    if not list(df['image_name'].astype(str))[0].endswith('.jpg'):\n        df['image_name']+='.jpg' \n    #one hot encoding of anatomical features\n    columns = ['image_name','sex', 'age_approx', 'anatom_site_general_challenge']\n    if type != 'test':\n        columns = ['target']+columns\n        df['target'] = df['target'].astype(str)\n    df = df[columns]\n    df['sex'].replace({'male':1, 'female':0}, inplace = True)\n    df['anatom_site_general_challenge'].replace({np.nan:'nan'}, inplace = True)\n    anatomic_sites = ['upper extremity', 'oral/genital', 'palms/soles', 'torso', 'head/neck', 'lower extremity', 'nan']\n    for btype in anatomic_sites:\n        df[btype] = df['anatom_site_general_challenge'] == btype\n        df[btype] = df[btype].astype(int)\n    df.drop(['anatom_site_general_challenge'], inplace = True, axis = 1)\n    #z-transform age\n    df['age_approx'] = (df['age_approx']-np.mean(df['age_approx']))/np.std(df['age_approx'])\n    df.dropna(inplace = True)\n    return df.sample(len(df)) #shuffle dataframe\n\ntest_info = normalize_df(basepath + 'test.csv')\nseg_info = normalize_df(basepath + \"train.csv\", type = 'train')\n\n\n#undersample majority class\nmalignant  = seg_info[seg_info['target'] == '1']\nbenign = seg_info[seg_info['target'] == '0']\nbenign = benign.sample(len(malignant))\n\nseg_info = pd.concat([benign, malignant])\n\nskf = StratifiedKFold(n_splits=5) # split 80/20 into test/valid data\nfor train_index,  val_index in skf.split(np.zeros(len(seg_info)), seg_info['target']):\n    break\ndel skf\n\n#shuffle dataframes\ntrain_df = seg_info.iloc[train_index].sample(len(train_index))\nval_df = seg_info.iloc[val_index].sample(len(val_index))\n\nprint(val_df['target'].value_counts())\nprint(train_df['target'].value_counts())\n\nseg_train_generator = seg_gen.flow_from_dataframe(\n    dataframe = train_df,\n    directory = basepath + 'jpeg/train/',\n    x_col = 'image_name',\n    y_col = 'target',\n    target_size =(x, y), # resize images\n    batch_size = batch_size,\n    validate_filenames = False,\n    shuffle = False,#preserve order in df to access demographic info\n    class_mode = 'binary'\n)\n\nseg_valid_generator = seg_gen.flow_from_dataframe(\n    dataframe = val_df,\n    directory = basepath + 'jpeg/train/',\n    x_col = 'image_name',\n    y_col = 'target',\n    target_size =(x, y),\n    batch_size = valid_size,\n    validate_filenames = False,\n    shuffle = False,\n    class_mode = 'binary'\n)\n\nseg_test_generator = seg_gen.flow_from_dataframe(\n    dataframe = test_info,\n    directory = basepath + 'jpeg/test/',\n    x_col = 'image_name',\n    y_col = None,\n    target_size =(x, y),\n    batch_size = 17,\n    validate_filenames = False,\n    shuffle = False,\n    class_mode = None\n)\n\nsegmentation_model = load_model('../input/bdcunet/weight_isic18.hdf5')\nprediction_model = build_prediction_model()\n\ndef rotate_image(image, angle, image_center):\n    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n    result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n    return result\n\ndef get_asymmetry_from_contours(img, big_contours):\n    major_axis_lengths = []\n    minor_axis_lengths = []\n    asymmetry_vs = []\n    asymmetry_hs = []\n    for idx, bc in enumerate(big_contours): #get asymmetry metrics for each sufficiently large contour\n        ellipse = cv2.fitEllipse(bc)\n        \n        center = ellipse[0]\n        rotation = ellipse[2]\n        width = ellipse[1][0]\n        height = ellipse[1][1]\n        \n        major_axis_lengths.append(height)\n        minor_axis_lengths.append(width)        \n        \n        #draw contour onto a mask\n        mask = np.zeros_like(img)\n        out = np.zeros_like(img) \n        cv2.drawContours(mask, big_contours, idx, 1, -1)    \n        out[mask == 1] = img[mask == 1]\n        \n        #crop and rotate to contour mask\n        rot = rotate_image(out, rotation, center)\n        x,y,w,h = cv2.boundingRect(rot)\n        rot_crop = rot[y:y+h, x:x+w]\n        \n        _, rot_crop = cv2.threshold(rot_crop, 0, 1, cv2.THRESH_BINARY) #everything greater than zero as mask       \n\n        floodfill = np.pad(rot_crop.copy(), pad_width = 1, mode= 'constant', constant_values = 0)\n        cv2.floodFill(floodfill, None,  (0,0), 2) #fill with 2s - all interior holes will still be zeros\n        floodfill[np.where(floodfill == 0)] = 1 #fills in all interior holes\n        floodfill = cv2.bitwise_not(floodfill)\n        \n        mask_vflip = cv2.flip(floodfill, 0)\n        mask_hflip = cv2.flip(floodfill, 1)\n        \n        #compare overlap of horizontal/vertical flips\n        vflip_or_floodfill = np.sum(cv2.bitwise_or(mask_vflip, floodfill))\n        hflip_or_floodfill = np.sum(cv2.bitwise_or(mask_hflip, floodfill)) \n        if vflip_or_floodfill > 0:\n            asymmetry_v = np.sum(cv2.bitwise_and(mask_vflip, floodfill))/vflip_or_floodfill\n        else:\n            asymmetry_v = 0\n            \n        if hflip_or_floodfill > 0: # no overlap\n            asymmetry_h = np.sum(cv2.bitwise_and(mask_hflip, floodfill))/hflip_or_floodfill\n        else:\n            asymmetry_h = 0\n        \n        asymmetry_vs.append(asymmetry_v)\n        asymmetry_hs.append(asymmetry_h)\n        \n    return major_axis_lengths, minor_axis_lengths, asymmetry_vs, asymmetry_hs\n\ndef get_asymmetry(img):\n    contours,hierarchy = cv2.findContours(img, 1, cv2.CHAIN_APPROX_NONE)\n    \n    contour_areas = np.array([cv2.contourArea(c) for c in contours])\n    if np.sum(contour_areas > 0):\n        proportional_contour_areas = contour_areas/np.sum(contour_areas)\n    else:\n        print('No contours found, returning all-zero asymmetry')\n        return [0,0,0,0,0,0,0,0] \n    #extract contours of interest as all contours with >10% of contour area\n    big_indices = np.where(proportional_contour_areas > 0.1)\n    big_contours = np.array(contours)[big_indices] # select contours that have > 10% of the total area of all contours\n    big_contour_areas = np.array(contour_areas)[big_indices]\n\n    perimeters = [cv2.arcLength(c, True) for c in big_contours]\n    \n    major_length, minor_length, asymmetry_vs, asymmetry_hs = get_asymmetry_from_contours(img, big_contours)\n       \n    a_p_ratio = []\n    compactness_index = []\n    p_times_a = []\n    \n    for a, p in zip(big_contour_areas, perimeters):\n        if p == 0:\n            p = 1\n            print('0 perimeter')\n        a_p_ratio.append(a/p)\n        compactness_index.append((4*np.pi*a)/p**2)\n        p_times_a.append(p*a)\n        \n    d1_primes = [np.sqrt((4*a)/np.pi) for a in big_contour_areas]\n    d1_double_primes = [(D+d)/2 for D, d in zip(major_length, minor_length)]\n    d1 = np.mean([(d1_prime + d1_double_prime)/2 for d1_prime, d1_double_prime in zip(d1_primes, d1_double_primes)])\n    d2 = np.mean([np.abs(D-d) for D, d in zip(major_length, minor_length)])\n    features = [np.mean(a_p_ratio), np.mean(compactness_index), np.mean(p_times_a), np.mean(asymmetry_vs), np.mean(asymmetry_hs), len(big_contour_areas),  d1, d2]\n    return features\n\ndef extract_features(imgs,masks, filenames, train):\n    X = np.empty((imgs.shape[0], 785))\n    kernel = np.ones((1,1),np.uint8)\n    \n    #load df for filename/demographic ingo\n    if train == 'train':\n        df = train_df\n    elif train == 'valid':\n        df = val_df\n    else:\n        df = test_info \n        \n    i = 0   \n    for mask, img, filename in zip(masks, imgs, filenames):\n        #turn BCDU mask into binary mask\n        ret, thresh = cv2.threshold(mask, np.mean(mask.flatten())*1.1, 255, 0)\n        mask = cv2.dilate(thresh,kernel,iterations=10)\n\n        if np.sum(mask)==0: # prevent masking with all 0 mask -> segmentation failed\n            mask=np.ones(mask.shape)\n            asymmetry = [0,0,0,0,0,1, 0 , 0]\n            print(\"Segmentation failed\")\n            hist = np.array([cv2.calcHist([img], [j], None, [256],  [0,256]) for j in range(3)]).flatten()\n\n        else:\n            mask =np.uint8(mask.reshape(256,256))\n            asymmetry = get_asymmetry(mask)\n            #channel-wise histogram of pixel values\n            hist = np.array([cv2.calcHist([img], [j], mask, [256],  [0,256]) for j in range(3)]).flatten()\n    \n        demographics  = df.loc[df['image_name']== filename].values[0][-9:]\n        features = list(hist)+list(demographics)+asymmetry\n        X[i] = features\n        i+=1\n        \n    return X\n\ndef remove_hairs(image):\n    # convert image to grayScale\n    image = np.uint8(image)\n    grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    # kernel for morphologyEx\n    kernel = cv2.getStructuringElement(1,(17,17))\n    # apply MORPH_BLACKHAT to grayScale image\n    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n    # apply thresholding to blackhat\n    _,threshold = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n    # inpaint with original image and threshold image\n    final_image = cv2.inpaint(image,threshold,1,cv2.INPAINT_TELEA)\n    return final_image\n\n# %% [code]\nt1 = time.time()\nepochs = 100\npatience = 25\n\nbest_val_auc = 0\nepochs_since_best =0\nhistory_across_epochs = {}\nvalidation_history = {}\nfor e in range(epochs):\n    print('Epoch', e)\n    \n    #TRAIN\n    batches = 0 \n    n_train_iter = len(train_df)//seg_train_generator.batch_size\n    bh_loss = []\n    bh_auc = []\n    bh_prec = []\n    bh_rec = []\n    bh_acc = []\n    for X_train, y_train in seg_train_generator: # generator sampled images\n        if batches >= n_train_iter:\n            break\n        X_train = np.array([remove_hairs(x) for x in X_train])\n        \n        idx = (seg_train_generator.batch_index - 1) * seg_train_generator.batch_size\n        filenames= seg_train_generator.filenames[idx : idx + seg_train_generator.batch_size]\n        #process masks to get just melanoma from images       \n        segmented_masks_train = segmentation_model.predict(X_train)\n        X_train = extract_features(X_train, segmented_masks_train, filenames, train = 'train')\n        \n        history = prediction_model.fit(X_train, y_train, verbose = 0, batch_size = X_train.shape[0]) \n        \n        #collect training metriics\n        bh_loss.append(history.history['loss'])\n        bh_auc.append(history.history['auc'])\n        bh_acc.append(history.history['accuracy'])\n        bh_prec.append(history.history['precision'])\n        bh_rec.append(history.history['recall'])      \n        \n        batches +=1\n        \n    history_across_epochs[e] = {'loss':np.nanmean(bh_loss), 'auc':np.nanmean(bh_auc), 'acc':np.nanmean(bh_acc), \n                     'prec':np.nanmean(bh_prec),'rec':np.nanmean(bh_rec)}\n    print('Training Loss:', history_across_epochs[e]['loss'], 'Training AUC', history_across_epochs[e]['auc'], round(time.time()-t1, 2))\n    t1 = time.time()\n    \n    #VALIDATE AND ASSESS\n\n    \n    valid_batches = 0\n\n    loss_fnctn = BinaryCrossentropy()\n    n_val_iter = len(val_df)//seg_valid_generator.batch_size\n    \n    all_pred = []\n    \n    all_true = []\n    for X_valid, y_valid in seg_valid_generator: # generator sampled images\n        if valid_batches >= n_val_iter:\n            break\n        #process masks to get just melanoma from images \n        X_valid =  np.array([remove_hairs(x) for x in X_valid])\n\n        segmented_masks_valid = segmentation_model.predict(X_valid)\n        \n        idx = (seg_valid_generator.batch_index - 1) * seg_valid_generator.batch_size\n        filenames= seg_valid_generator.filenames[idx : idx + seg_valid_generator.batch_size]\n        \n        X_valid= extract_features(X_valid, segmented_masks_valid, filenames, train='valid')\n        \n        y_pred = list(prediction_model.predict(X_valid).flatten())\n        all_pred += y_pred\n\n        all_true += list(y_valid)\n        \n        valid_batches +=1\n\n        collect()\n        \n    loss = loss_fnctn(FloatTensor(all_true), FloatTensor(all_pred))\n    \n    print(np.sum(all_true), np.sum(all_pred))\n    if np.sum(all_true) > 0 and np.sum(all_pred) > 0:\n        auc = roc_auc_score(all_true, all_pred)\n    else:\n        auc = np.nan\n \n    y_pred_accuracy = np.array([0.0  if i <0.5 else 1.0 for i in all_pred])\n    accuracy = np.sum(all_true == y_pred_accuracy)/len(y_pred_accuracy) \n        \n    print('Val loss:', loss, 'Val AUC:',  auc, 'Val Accuracy:', accuracy)\n    \n    validation_history[e] = {'loss':loss, 'auc':auc, 'acc':accuracy}\n    \n    if auc > best_val_auc:\n        print('Validation AUC improved from best value. Saving to best_model.h5')\n        best_val_auc = auc\n        prediction_model.save('best_model.h5')\n        epochs_since_best = 0\n    else:\n        epochs_since_best +=1 \n        print('Validation AUC did not improve from', best_val_auc)\n        \n        \n    if epochs_since_best > patience:\n        print('No improvement in ', patience,  'epochs. Quitting.')\n        break \n    \n    collect()\n\nprint(validation_history)\ndf = pd.DataFrame(validation_history)\nprint(df)\ndf.to_csv('Validationhistory.csv')\n\nprint(history_across_epochs)\ndf2 = pd.DataFrame(history_across_epochs)\nprint(df2)\ndf2.to_csv('Trainhistory.csv')\n","execution_count":3,"outputs":[{"output_type":"stream","text":"0    117\n1    117\nName: target, dtype: int64\n0    467\n1    467\nName: target, dtype: int64\nFound 934 non-validated image filenames belonging to 2 classes.\nFound 234 non-validated image filenames belonging to 2 classes.\nFound 10982 non-validated image filenames.\nEpoch 0\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'collect' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-78382aadb9cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m#process masks to get just melanoma from images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0msegmented_masks_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmentation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmented_masks_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-78382aadb9cd>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(imgs, masks, filenames, train)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'collect' is not defined"]}]}],"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}